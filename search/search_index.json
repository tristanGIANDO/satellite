{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Hi there!</p> <p>I'm Tristan and I'm doing this project to learn how to use satellite images and deepen my knowledge of deep learning.</p> <ul> <li>This is my first experience with PyTorch (I was using Tensorflow)</li> <li>First experience with real satellite data (Sentinel-2)</li> </ul> <p></p>"},{"location":"data/","title":"Data Preparation","text":""},{"location":"data/#download-and-analyse-the-training-dataset","title":"Download and analyse the training dataset","text":"<p>From this dataset, we create a CSV dataset containing the paths of all files downloaded locally (8400 lines).</p> <p>Here are the channels available for an image: </p> <p>We want to identify clouds on RGB channels using the TARGET mask.</p> <p>Here's a compiled RGB image (although each channel will be separated for the model):</p> <p></p> <p>And here's a compiled RGBA image with the clouds removed (this will be the post-processed output image):</p> <p></p>"},{"location":"data/#download-and-analyze-sentinel-2-data","title":"Download and analyze Sentinel-2 data","text":"<p>We download the data from the public S3 bucket. These are JPEG2000 images, so we need to use <code>rasterio</code> rather than <code>Pillow</code>.</p> <p><code>rasterio</code> is designed for :</p> <ul> <li>GeoTIFF / JP2 reading</li> <li>Geospatial metadata (CRS, bbox), access to coordinates, UTM, not <code>Pillow</code>.</li> <li>Multi-band images (e.g.: 13 bands) handles band stacks perfectly, <code>Pillow</code> only understands RGB / L / P</li> <li>Support for NIR, SWIR, 16-bit+ bands, whereas <code>Pillow</code> is limited to 8-bit</li> <li>GIS interoperability (QGIS, GDAL)</li> </ul> channel sentinel code R B04 G B03 B B02 Near Infrared B08 <p>Each Sentinel image can be found thanks to an ID that depends on latitude and longitude (see examples of tiles below).</p> <p></p> <p>These are some examples of B04, B03, B02, B08 channels for 2 tiles:  </p>"},{"location":"inference/","title":"Inference pipeline","text":"<p>We start by selecting a time range from which we will create our composite image. For example, we would like to use all Sentinel-2 captures from May 2025 in Paris.</p> <p>We also choose an image to serve as a color reference. It doesn't necessarily have to be from the same geographic area, the important thing is that the colors are similar. So, we shouldn't use an ocean image as a reference for a forest image.</p>"},{"location":"inference/#preprocessing","title":"Preprocessing","text":"<p>After loading the Sentinel-2 images, we apply several preprocessing steps. Since the images were captured at different times and on different days, brightness and colors are never consistent.</p> <p>We start by matching the histograms of the images to that of the reference image.</p> <p></p> <p>Then, we apply a contrast stretching to the input image using the 2nd and 98th percentiles. This method rescales the pixel values of the input image so that the 2nd percentile maps to 0 and the 98th percentile maps to 1, with values outside this range clipped accordingly. This enhances the contrast of the image by reducing the effect of outliers.</p> <p>Finally, we apply the Gray World color balance algorithm to a stacked image. It assumes that the average color of a scene is gray, and adjusts each channel so that their averages are equal, effectively correcting color casts in the image.</p>"},{"location":"inference/#split-into-tiles","title":"Split into tiles","text":"<p>Each image has a <code>10980*10980px</code> size whereas the model has been trained on <code>384*384px</code> images. We have to split it into small squares (tiles) and then give it to the model.</p> <p>To do this, we create a tile grid that will serve both as a cutting template for all the images and as a template for reconstructing them.</p>"},{"location":"inference/#inference","title":"Inference","text":"<p>The predicted mask for an image:</p> <p></p> <p>The goal is now to keep the cloudless tiles only.</p>"},{"location":"inference/#filter-tiles","title":"Filter tiles","text":"<p>To filter out cloudy tiles, we keep only those that have less than a certain percentage of pixels considered white, and we add the RGB tile to the reconstruction grid.</p> <p>Each time a tile is added to the grid, we calculate the number of remaining tiles to process in order to stop the inference once the entire image has been completed.</p>"},{"location":"inference/#results","title":"Results","text":""},{"location":"inference/#paris-august-2024-may-2025","title":"Paris: August 2024 - May 2025","text":""},{"location":"training/","title":"Model Training","text":""},{"location":"training/#create-a-custom-u-net","title":"Create a custom U-Net","text":"<p>A U-Net model is a type of convolutional neural network designed specifically for image segmentation tasks.</p> <p>There are two main parts:</p> <ol> <li> <p>Encoder:    Convolution series + max pooling which progressively reduces the image resolution while increasing the number of channels to understand the image context.</p> </li> <li> <p>Decoder:    Series of transposed convolutions (or upsampling) that increase resolution.</p> </li> </ol> <p>The result is a binary mask. In our case, this will be the pixels that correspond to clouds.</p>"},{"location":"training/#baseline-simple-u-net-with-3-in_channels-r-g-b","title":"Baseline (Simple U-net with 3 in_channels (R, G, B))","text":"<p>Let's train with this:</p> <pre><code>SimpleUNet(\n  (enc1): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (enc2): DoubleConv(\n    (conv): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (bottleneck): DoubleConv(\n    (conv): Sequential(\n...\n      (5): ReLU(inplace=True)\n    )\n  )\n  (final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n)\n</code></pre> <p>Results for a first training of 10 epochs with only 100 images:</p> <p> </p> <p>Results for a second training of 9 epochs with 4200 images:</p> <p> </p> <p>This very first model doesn't do too badly, although unfortunately, due to an error in the pipeline, I have to abandon it.</p> <p>In addition, it was only based on the R, G, B layers. I had assumed that Sentinel-2 data would be quite difficult to retrieve, so I wouldn't have the NIR (near infrared) layer. However, it was actually quite simple, so I redid the whole thing.</p>"},{"location":"training/#u-net-with-4-in_channels-r-g-b-nir","title":"U-net with 4 in_channels (R, G, B, NIR)","text":"<p>This U-net is based on channels R, G, B, NIR.</p> <p>As we're dealing with binary segmentation (each pixel is classified as \"cloudy\" or \"not cloudy\"), we can use the <code>BCEWithLogitsLoss</code> loss function. This is a function that applies a sigmoid to the model output and then a Binary Cross-Entropy between the target and the ground truth.</p> <p>Remember: the sigmoid is a symmetrical S-curve centered on 0.5, which crushes the extreme values. It is used to calculate probabilities between 0 and 1. Binary Cross-Entropy measures the error between a predicted probability and an actual target value. The smaller the error, the better.</p> <pre><code>SimpleUNetV2(\n  (enc1): Sequential(\n    (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n  )\n  (enc2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n  )\n  (bottleneck): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n  )\n  (dropout_bottleneck): Dropout2d(p=0.3, inplace=False)\n  (up1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n  (dec1): Sequential(\n    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n  )\n  (dropout_dec1): Dropout2d(p=0.3, inplace=False)\n  (up2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n  (dec2): Sequential(\n    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n  )\n  (final): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n</code></pre>"},{"location":"training/#first-experiment","title":"First experiment","text":"<p>I'm very surprised at the quality of the prediction, given that I've only given a subset of 100 lines (some of which are completely empty) out of 10 epochs. However, it's clear that he's still confusing snow-capped mountains with clouds.</p>"},{"location":"training/#second-experiment","title":"Second experiment","text":""}]}